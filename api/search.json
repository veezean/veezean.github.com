[{"id":"59d1635fe0da833e1ed3dc6653448adf","title":"全面吃透JAVA Stream流操作，让代码更加的优雅","content":"在JAVA中，涉及到对 数组、Collection等集合类中的元素进行操作的时候，通常会通过循环的方式进行逐个处理，或者使用Stream的方式进行处理。\n例如，现在有这么一个需求：\n\n\n\n\n\n\n\n\n\n从给定句子中返回单词长度大于5的单词列表，按长度倒序输出，最多返回3个\n在JAVA7及之前的代码中，我们会可以照如下的方式进行实现：\n\n&#x2F;**\n * 【常规方式】\n * 从给定句子中返回单词长度大于5的单词列表，按长度倒序输出，最多返回3个\n *\n * @param sentence 给定的句子，约定非空，且单词之间仅由一个空格分隔\n * @return 倒序输出符合条件的单词列表\n *&#x2F;\npublic List&lt;String&gt; sortGetTop3LongWords(@NotNull String sentence) &#123;\n    &#x2F;&#x2F; 先切割句子，获取具体的单词信息\n    String[] words &#x3D; sentence.split(&quot; &quot;);\n    List&lt;String&gt; wordList &#x3D; new ArrayList&lt;&gt;();\n    &#x2F;&#x2F; 循环判断单词的长度，先过滤出符合长度要求的单词\n    for (String word : words) &#123;\n        if (word.length() &gt; 5) &#123;\n            wordList.add(word);\n        &#125;\n    &#125;\n    &#x2F;&#x2F; 对符合条件的列表按照长度进行排序\n    wordList.sort((o1, o2) -&gt; o2.length() - o1.length());\n    &#x2F;&#x2F; 判断list结果长度，如果大于3则截取前三个数据的子list返回\n    if (wordList.size() &gt; 3) &#123;\n        wordList &#x3D; wordList.subList(0, 3);\n    &#125;\n    return wordList;\n&#125;\n\n\n在JAVA8及之后的版本中，借助Stream流，我们可以更加优雅的写出如下代码：\n\n&#x2F;**\n * 【Stream方式】\n * 从给定句子中返回单词长度大于5的单词列表，按长度倒序输出，最多返回3个\n *\n * @param sentence 给定的句子，约定非空，且单词之间仅由一个空格分隔\n * @return 倒序输出符合条件的单词列表\n *&#x2F;\npublic List&lt;String&gt; sortGetTop3LongWordsByStream(@NotNull String sentence) &#123;\n    return Arrays.stream(sentence.split(&quot; &quot;))\n            .filter(word -&gt; word.length() &gt; 5)\n            .sorted((o1, o2) -&gt; o2.length() - o1.length())\n            .limit(3)\n            .collect(Collectors.toList());\n&#125;\n\n\n直观感受上，Stream的实现方式代码更加简洁、一气呵成。很多的同学在代码中也经常使用Stream流，但是对Stream流的认知往往也是仅限于会一些简单的 filter、map、collect等操作，但JAVA的Stream可以适用的场景与能力远不止这些。\n\n那么问题来了：Stream相较于传统的foreach的方式处理，到底有啥优势？\n这里我们可以先搁置这个问题，先整体全面的了解下Stream，然后再来讨论下这个问题。\n笔者结合在团队中多年的代码检视遇到的情况，结合平时项目编码实践经验，对Stream的核心要点与易混淆用法、典型使用场景等进行了详细的梳理总结，希望可以帮助大家对Stream有个更全面的认知，也可以更加高效的应用到项目开发中去。\nStream初相识概括讲，可以将Stream流操作分为3种类型：\n\n创建Stream\nStream中间处理\n终止Steam\n\n\n每个Stream管道操作类型都包含若干API方法，先列举下各个API方法的功能介绍。\n\n开始管道\n\n主要负责新建一个Stream流，或者基于现有的数组、List、Set、Map等集合类型对象创建出新的Stream流。\n\n\n\nAPI\n功能说明\n\n\n\nstream()\n创建出一个新的stream串行流对象\n\n\nparallelStream()\n创建出一个可并行执行的stream流对象\n\n\nStream.of()\n通过给定的一系列元素创建一个新的Stream串行流对象\n\n\n\n\n中间管道\n\n负责对Stream进行处理操作，并返回一个新的Stream对象，中间管道操作可以进行叠加。\n\n\n\nAPI\n功能说明\n\n\n\nfilter()\n按照条件过滤符合要求的元素， 返回新的stream流\n\n\nmap()\n将已有元素转换为另一个对象类型，一对一逻辑，返回新的stream流\n\n\nflatMap()\n将已有元素转换为另一个对象类型，一对多逻辑，即原来一个元素对象可能会转换为1个或者多个新类型的元素，返回新的stream流\n\n\nlimit()\n仅保留集合前面指定个数的元素，返回新的stream流\n\n\nskip()\n跳过集合前面指定个数的元素，返回新的stream流\n\n\nconcat()\n将两个流的数据合并起来为1个新的流，返回新的stream流\n\n\ndistinct()\n对Stream中所有元素进行去重，返回新的stream流\n\n\nsorted()\n对stream中所有的元素按照指定规则进行排序，返回新的stream流\n\n\npeek()\n对stream流中的每个元素进行逐个遍历处理，返回处理后的stream流\n\n\n\n\n终止管道\n\n顾名思义，通过终止管道操作之后，Stream流将会结束，最后可能会执行某些逻辑处理，或者是按照要求返回某些执行后的结果数据。\n\n\n\nAPI\n功能说明\n\n\n\ncount()\n返回stream处理后最终的元素个数\n\n\nmax()\n返回stream处理后的元素最大值\n\n\nmin()\n返回stream处理后的元素最小值\n\n\nfindFirst()\n找到第一个符合条件的元素时则终止流处理\n\n\nfindAny()\n找到任何一个符合条件的元素时则退出流处理，这个对于串行流时与findFirst相同，对于并行流时比较高效，任何分片中找到都会终止后续计算逻辑\n\n\nanyMatch()\n返回一个boolean值，类似于isContains(),用于判断是否有符合条件的元素\n\n\nallMatch()\n返回一个boolean值，用于判断是否所有元素都符合条件\n\n\nnoneMatch()\n返回一个boolean值， 用于判断是否所有元素都不符合条件\n\n\ncollect()\n将流转换为指定的类型，通过Collectors进行指定\n\n\ntoArray()\n将流转换为数组\n\n\niterator()\n将流转换为Iterator对象\n\n\nforeach()\n无返回值，对元素进行逐个遍历，然后执行给定的处理逻辑\n\n\nStream方法使用map与flatMapmap与 flatMap都是用于转换已有的元素为其它元素，区别点在于：\n\nmap 必须是一对一的，即每个元素都只能转换为1个新的元素\nflatMap 可以是一对多的，即每个元素都可以转换为1个或者多个新的元素\n\n\n比如：有一个字符串ID列表，现在需要将其转为User对象列表。可以使用map来实现：\n\n&#x2F;**\n * 演示map的用途：一对一转换\n *&#x2F;\npublic void stringToIntMap() &#123;\n    List&lt;String&gt; ids &#x3D; Arrays.asList(&quot;205&quot;, &quot;105&quot;, &quot;308&quot;, &quot;469&quot;, &quot;627&quot;, &quot;193&quot;, &quot;111&quot;);\n    &#x2F;&#x2F; 使用流操作\n    List&lt;User&gt; results &#x3D; ids.stream()\n            .map(id -&gt; &#123;\n                User user &#x3D; new User();\n                user.setId(id);\n                return user;\n            &#125;)\n            .collect(Collectors.toList());\n    System.out.println(results);\n&#125;\n\n\n执行之后，会发现每一个元素都被转换为对应新的元素，但是前后总元素个数是一致的：\n\n[User&#123;id&#x3D;&#39;205&#39;&#125;, \n User&#123;id&#x3D;&#39;105&#39;&#125;,\n User&#123;id&#x3D;&#39;308&#39;&#125;, \n User&#123;id&#x3D;&#39;469&#39;&#125;, \n User&#123;id&#x3D;&#39;627&#39;&#125;, \n User&#123;id&#x3D;&#39;193&#39;&#125;, \n User&#123;id&#x3D;&#39;111&#39;&#125;]\n\n\n\n再比如：现有一个句子列表，需要将句子中每个单词都提取出来得到一个所有单词列表。这种情况用map就搞不定了，需要 flatMap上场了：\n\npublic void stringToIntFlatmap() &#123;\n    List&lt;String&gt; sentences &#x3D; Arrays.asList(&quot;hello world&quot;,&quot;Jia Gou Wu Dao&quot;);\n    &#x2F;&#x2F; 使用流操作\n    List&lt;String&gt; results &#x3D; sentences.stream()\n            .flatMap(sentence -&gt; Arrays.stream(sentence.split(&quot; &quot;)))\n            .collect(Collectors.toList());\n    System.out.println(results);\n&#125;\n\n\n执行结果如下，可以看到结果列表中元素个数是比原始列表元素个数要多的：\n\n[hello, world, Jia, Gou, Wu, Dao]\n\n\n这里需要补充一句，flatMap操作的时候其实是先每个元素处理并返回一个新的Stream，然后将多个Stream展开合并为了一个完整的新的Stream，如下：\n\npeek和foreach方法peek和 foreach，都可以用于对元素进行遍历然后逐个的进行处理。\n但根据前面的介绍，peek属于中间方法，而foreach属于终止方法。这也就意味着peek只能作为管道中途的一个处理步骤，而没法直接执行得到结果，其后面必须还要有其它终止操作的时候才会被执行；而foreach作为无返回值的终止方法，则可以直接执行相关操作。\n\npublic void testPeekAndforeach() &#123;\n    List&lt;String&gt; sentences &#x3D; Arrays.asList(&quot;hello world&quot;,&quot;Jia Gou Wu Dao&quot;);\n    &#x2F;&#x2F; 演示点1： 仅peek操作，最终不会执行\n    System.out.println(&quot;----before peek----&quot;);\n    sentences.stream().peek(sentence -&gt; System.out.println(sentence));\n    System.out.println(&quot;----after peek----&quot;);\n    &#x2F;&#x2F; 演示点2： 仅foreach操作，最终会执行\n    System.out.println(&quot;----before foreach----&quot;);\n    sentences.stream().forEach(sentence -&gt; System.out.println(sentence));\n    System.out.println(&quot;----after foreach----&quot;);\n    &#x2F;&#x2F; 演示点3： peek操作后面增加终止操作，peek会执行\n    System.out.println(&quot;----before peek and count----&quot;);\n    sentences.stream().peek(sentence -&gt; System.out.println(sentence)).count();\n    System.out.println(&quot;----after peek and count----&quot;);\n&#125;\n\n\n输出结果可以看出，peek独自调用时并没有被执行、但peek后面加上终止操作之后便可以被执行，而foreach可以直接被执行：\n\n----before peek----\n----after peek----\n----before foreach----\nhello world\nJia Gou Wu Dao\n----after foreach----\n----before peek and count----\nhello world\nJia Gou Wu Dao\n----after peek and count----\n\n\n\n\nfilter、sorted、distinct、limit这几个都是常用的Stream的中间操作方法，具体的方法的含义在上面的表格里面有说明。具体使用的时候，可以根据需要选择一个或者多个进行组合使用，或者同时使用多个相同方法的组合：\n\npublic void testGetTargetUsers() &#123;\n    List&lt;String&gt; ids &#x3D; Arrays.asList(&quot;205&quot;,&quot;10&quot;,&quot;308&quot;,&quot;49&quot;,&quot;627&quot;,&quot;193&quot;,&quot;111&quot;, &quot;193&quot;);\n    &#x2F;&#x2F; 使用流操作\n    List&lt;Dept&gt; results &#x3D; ids.stream()\n            .filter(s -&gt; s.length() &gt; 2)\n            .distinct()\n            .map(Integer::valueOf)\n            .sorted(Comparator.comparingInt(o -&gt; o))\n            .limit(3)\n            .map(id -&gt; new Dept(id))\n            .collect(Collectors.toList());\n    System.out.println(results);\n&#125;\n\n\n上面的代码片段的处理逻辑很清晰：\n\n使用filter过滤掉不符合条件的数据\n通过distinct对存量元素进行去重操作\n通过map操作将字符串转成整数类型\n借助sorted指定按照数字大小正序排列\n使用limit截取排在前3位的元素\n又一次使用map将id转为Dept对象类型\n使用collect终止操作将最终处理后的数据收集到list中\n\n输出结果：\n[Dept&#123;id&#x3D;111&#125;,  Dept&#123;id&#x3D;193&#125;,  Dept&#123;id&#x3D;205&#125;]\n\n\n\n简单结果终止方法按照前面介绍的，终止方法里面像 count、max、min、findAny、findFirst、anyMatch、allMatch、noneMatch等方法，均属于这里说的简单结果终止方法。所谓简单，指的是其结果形式是数字、布尔值或者Optional对象值等。\n\npublic void testSimpleStopOptions() &#123;\n    List&lt;String&gt; ids &#x3D; Arrays.asList(&quot;205&quot;, &quot;10&quot;, &quot;308&quot;, &quot;49&quot;, &quot;627&quot;, &quot;193&quot;, &quot;111&quot;, &quot;193&quot;);\n    &#x2F;&#x2F; 统计stream操作后剩余的元素个数\n    System.out.println(ids.stream().filter(s -&gt; s.length() &gt; 2).count());\n    &#x2F;&#x2F; 判断是否有元素值等于205\n    System.out.println(ids.stream().filter(s -&gt; s.length() &gt; 2).anyMatch(&quot;205&quot;::equals));\n    &#x2F;&#x2F; findFirst操作\n    ids.stream().filter(s -&gt; s.length() &gt; 2)\n            .findFirst()\n            .ifPresent(s -&gt; System.out.println(&quot;findFirst:&quot; + s));\n&#125;\n\n\n执行后结果为：\n\n6\ntrue\nfindFirst:205\n\n\n\n避坑提醒\n这里需要补充提醒下，一旦一个Stream被执行了终止操作之后，后续便不可以再读这个流执行其他的操作了，否则会报错，看下面示例：\n\npublic void testHandleStreamAfterClosed() &#123;\n    List&lt;String&gt; ids &#x3D; Arrays.asList(&quot;205&quot;, &quot;10&quot;, &quot;308&quot;, &quot;49&quot;, &quot;627&quot;, &quot;193&quot;, &quot;111&quot;, &quot;193&quot;);\n    Stream&lt;String&gt; stream &#x3D; ids.stream().filter(s -&gt; s.length() &gt; 2);\n    &#x2F;&#x2F; 统计stream操作后剩余的元素个数\n    System.out.println(stream.count());\n    System.out.println(&quot;-----下面会报错-----&quot;);\n    &#x2F;&#x2F; 判断是否有元素值等于205\n    try &#123;\n        System.out.println(stream.anyMatch(&quot;205&quot;::equals));\n    &#125; catch (Exception e) &#123;\n        e.printStackTrace();\n    &#125;\n    System.out.println(&quot;-----上面会报错-----&quot;);\n&#125;\n\n\n执行的时候，结果如下：\n\n6\n-----下面会报错-----\njava.lang.IllegalStateException: stream has already been operated upon or closed\n\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:229)\n\tat java.util.stream.ReferencePipeline.anyMatch(ReferencePipeline.java:449)\n\tat com.veezean.skills.stream.StreamService.testHandleStreamAfterClosed(StreamService.java:153)\n\tat com.veezean.skills.stream.StreamService.main(StreamService.java:176)\n-----上面会报错-----\n\n\n因为stream已经被执行 count()终止方法了，所以对stream再执行 anyMatch方法的时候，就会报错 stream has already been operated upon or closed，这一点在使用的时候需要特别注意。\n\n结果收集终止方法因为Stream主要用于对集合数据的处理场景，所以除了上面几种获取简单结果的终止方法之外，更多的场景是获取一个集合类的结果对象，比如List、Set或者HashMap等。\n这里就需要 collect方法出场了，它可以支持生成如下类型的结果数据：\n\n一个 集合类，比如List、Set或者HashMap等\nStringBuilder对象，支持将多个 字符串进行拼接处理并输出拼接后结果\n一个可以记录个数或者计算总和的对象（数据批量运算统计）\n\n\n生成集合应该算是collect最常被使用到的一个场景了：\n\npublic void testCollectStopOptions() &#123;\n    List&lt;Dept&gt; ids &#x3D; Arrays.asList(new Dept(17), new Dept(22), new Dept(23));\n    &#x2F;&#x2F; collect成list\n    List&lt;Dept&gt; collectList &#x3D; ids.stream().filter(dept -&gt; dept.getId() &gt; 20)\n            .collect(Collectors.toList());\n    System.out.println(&quot;collectList:&quot; + collectList);\n    &#x2F;&#x2F; collect成Set\n    Set&lt;Dept&gt; collectSet &#x3D; ids.stream().filter(dept -&gt; dept.getId() &gt; 20)\n            .collect(Collectors.toSet());\n    System.out.println(&quot;collectSet:&quot; + collectSet);\n    &#x2F;&#x2F; collect成HashMap，key为id，value为Dept对象\n    Map&lt;Integer, Dept&gt; collectMap &#x3D; ids.stream().filter(dept -&gt; dept.getId() &gt; 20)\n            .collect(Collectors.toMap(Dept::getId, dept -&gt; dept));\n    System.out.println(&quot;collectMap:&quot; + collectMap);\n&#125;\n\n\n结果如下：\n\ncollectList:[Dept&#123;id&#x3D;22&#125;, Dept&#123;id&#x3D;23&#125;]\ncollectSet:[Dept&#123;id&#x3D;23&#125;, Dept&#123;id&#x3D;22&#125;]\ncollectMap:&#123;22&#x3D;Dept&#123;id&#x3D;22&#125;, 23&#x3D;Dept&#123;id&#x3D;23&#125;&#125;\n\n\n\n生成拼接字符串将一个List或者数组中的值拼接到一个字符串里并以逗号分隔开，这个场景相信大家都不陌生吧？\n如果通过 for循环和 StringBuilder去循环拼接，还得考虑下最后一个逗号如何处理的问题，很繁琐:\n\npublic void testForJoinStrings() &#123;\n    List&lt;String&gt; ids &#x3D; Arrays.asList(&quot;205&quot;, &quot;10&quot;, &quot;308&quot;, &quot;49&quot;, &quot;627&quot;, &quot;193&quot;, &quot;111&quot;, &quot;193&quot;);\n    StringBuilder builder &#x3D; new StringBuilder();\n    for (String id : ids) &#123;\n        builder.append(id).append(&#39;,&#39;);\n    &#125;\n    &#x2F;&#x2F; 去掉末尾多拼接的逗号\n    builder.deleteCharAt(builder.length() - 1);\n    System.out.println(&quot;拼接后：&quot; + builder.toString());\n&#125;\n\n\n但是现在有了Stream，使用 collect可以轻而易举的实现：\n\npublic void testCollectJoinStrings() &#123;\n    List&lt;String&gt; ids &#x3D; Arrays.asList(&quot;205&quot;, &quot;10&quot;, &quot;308&quot;, &quot;49&quot;, &quot;627&quot;, &quot;193&quot;, &quot;111&quot;, &quot;193&quot;);\n    String joinResult &#x3D; ids.stream().collect(Collectors.joining(&quot;,&quot;));\n    System.out.println(&quot;拼接后：&quot; + joinResult);\n&#125;\n\n\n两种方式都可以得到完全相同的结果，但Stream的方式更优雅：\n拼接后：205,10,308,49,627,193,111,193\n\n\n📢 敲黑板：\n关于这里的说明，评论区中很多的小伙伴提出过疑问，就是这个场景其实使用 String.join() 就可以搞定了，并不需要上面使用 stream 的方式去实现。这里要声明下，Stream的魅力之处就在于其可以结合到其它的业务逻辑中进行处理，让代码逻辑更加的自然、一气呵成。如果纯粹是个String字符串拼接的诉求，确实没有必要使用Stream来实现，毕竟杀鸡焉用牛刀嘛~ 但是可以看看下面给出的这个示例，便可以感受出使用Stream进行字符串拼接的真正魅力所在。\n\n\n数据批量数学运算还有一种场景，实际使用的时候可能会比较少，就是使用collect生成数字数据的总和信息，也可以了解下实现方式：\n\npublic void testNumberCalculate() &#123;\n    List&lt;Integer&gt; ids &#x3D; Arrays.asList(10, 20, 30, 40, 50);\n    &#x2F;&#x2F; 计算平均值\n    Double average &#x3D; ids.stream().collect(Collectors.averagingInt(value -&gt; value));\n    System.out.println(&quot;平均值：&quot; + average);\n    &#x2F;&#x2F; 数据统计信息\n    IntSummaryStatistics summary &#x3D; ids.stream().collect(Collectors.summarizingInt(value -&gt; value));\n    System.out.println(&quot;数据统计信息： &quot; + summary);\n&#125;\n\n\n上面的例子中，使用collect方法来对list中元素值进行数学运算，结果如下：\n\n平均值：30.0\n总和： IntSummaryStatistics&#123;count&#x3D;5, sum&#x3D;150, min&#x3D;10, average&#x3D;30.000000, max&#x3D;50&#125;\n\n\n\n并行Stream机制说明使用并行流，可以有效利用计算机的多CPU硬件，提升逻辑的执行速度。并行流通过将一整个stream划分为 多个片段，然后对各个分片流并行执行处理逻辑，最后将各个分片流的执行结果汇总为一个整体流。\n\n约束与限制并行流类似于多线程在并行处理，所以与多线程场景相关的一些问题同样会存在，比如死锁等问题，所以在并行流终止执行的函数逻辑，必须要保证线程安全。\n\n回答最初的问题到这里，关于JAVA Stream的相关概念与用法介绍，基本就讲完了。我们再把焦点切回本文刚开始时提及的一个问题：\nStream相较于传统的foreach的方式处理stream，到底有啥优势？\n根据前面的介绍，我们应该可以得出如下几点答案：\n\n代码更简洁、偏声明式的编码风格，更容易体现出代码的逻辑意图\n逻辑间解耦，一个stream中间处理逻辑，无需关注上游与下游的内容，只需要按约定实现自身逻辑即可\n并行流场景效率会比迭代器逐个循环更高\n函数式接口，延迟执行的特性，中间管道操作不管有多少步骤都不会立即执行，只有遇到终止操作的时候才会开始执行，可以避免一些中间不必要的操作消耗\n\n当然了，Stream也不全是优点，在有些方面也有其弊端：\n\n代码调测debug不便\n程序员从历史写法切换到Stream时，需要一定的适应时间\n\n\n总结好啦，关于JAVA Stream的理解要点与使用技能的阐述就先到这里啦。那通过上面的介绍，各位小伙伴们是否已经跃跃欲试了呢？快去项目中使用体验下吧！当然啦，如果有疑问，也欢迎找我一起探讨探讨咯。\n此外：\n\n关于Stream中collect的分组、分片等进阶操作，以及对并行流的深入探讨，因为涉及内容比较多且相对独立，我会在后续的文档中展开专门介绍下，如果有兴趣的话，可以点个关注、避免迷路。\n关于本文中涉及的演示代码的完整示例，我已经整理并提交到github中，如果您有需要，可以自取：https://github.com/veezean/JavaBasicSkills\n\n\n我是悟道，聊技术、又不仅仅聊技术~\n如果觉得有用，请点个关注，也可以关注下我的公众号【架构悟道】，获取更及时的更新。\n期待与你一起探讨，一起成长为更好的自己。\n\n","slug":"2023/全面吃透JAVA Stream流操作，让代码更加的优雅","date":"2023-01-27T10:09:10.464Z","categories_index":"","tags_index":"","author_index":"Veezean"},{"id":"1b08d4f8c3a03d7692509fe1a83dbeac","title":"吧vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv","content":"在JAVA中，涉及到对 数组、Collection等集合类中的元素进行操作的时候，通常会通过循环的方式进行逐个处理，或者使用Stream的方式进行处理。\n例如，现在有这么一个需求：\n\n\n\n\n\n\n\n\n\n从给定句子中返回单词长度大于5的单词列表，按长度倒序输出，最多返回3个\n在JAVA7及之前的代码中，我们会可以照如下的方式进行实现：\n\n&#x2F;**\n * 【常规方式】\n * 从给定句子中返回单词长度大于5的单词列表，按长度倒序输出，最多返回3个\n *\n * @param sentence 给定的句子，约定非空，且单词之间仅由一个空格分隔\n * @return 倒序输出符合条件的单词列表\n *&#x2F;\npublic List&lt;String&gt; sortGetTop3LongWords(@NotNull String sentence) &#123;\n    &#x2F;&#x2F; 先切割句子，获取具体的单词信息\n    String[] words &#x3D; sentence.split(&quot; &quot;);\n    List&lt;String&gt; wordList &#x3D; new ArrayList&lt;&gt;();\n    &#x2F;&#x2F; 循环判断单词的长度，先过滤出符合长度要求的单词\n    for (String word : words) &#123;\n        if (word.length() &gt; 5) &#123;\n            wordList.add(word);\n        &#125;\n    &#125;\n    &#x2F;&#x2F; 对符合条件的列表按照长度进行排序\n    wordList.sort((o1, o2) -&gt; o2.length() - o1.length());\n    &#x2F;&#x2F; 判断list结果长度，如果大于3则截取前三个数据的子list返回\n    if (wordList.size() &gt; 3) &#123;\n        wordList &#x3D; wordList.subList(0, 3);\n    &#125;\n    return wordList;\n&#125;\n\n\n在JAVA8及之后的版本中，借助Stream流，我们可以更加优雅的写出如下代码：\n\n&#x2F;**\n * 【Stream方式】\n * 从给定句子中返回单词长度大于5的单词列表，按长度倒序输出，最多返回3个\n *\n * @param sentence 给定的句子，约定非空，且单词之间仅由一个空格分隔\n * @return 倒序输出符合条件的单词列表\n *&#x2F;\npublic List&lt;String&gt; sortGetTop3LongWordsByStream(@NotNull String sentence) &#123;\n    return Arrays.stream(sentence.split(&quot; &quot;))\n            .filter(word -&gt; word.length() &gt; 5)\n            .sorted((o1, o2) -&gt; o2.length() - o1.length())\n            .limit(3)\n            .collect(Collectors.toList());\n&#125;\n\n\n直观感受上，Stream的实现方式代码更加简洁、一气呵成。很多的同学在代码中也经常使用Stream流，但是对Stream流的认知往往也是仅限于会一些简单的 filter、map、collect等操作，但JAVA的Stream可以适用的场景与能力远不止这些。\n\n那么问题来了：Stream相较于传统的foreach的方式处理，到底有啥优势？\n这里我们可以先搁置这个问题，先整体全面的了解下Stream，然后再来讨论下这个问题。\n笔者结合在团队中多年的代码检视遇到的情况，结合平时项目编码实践经验，对Stream的核心要点与易混淆用法、典型使用场景等进行了详细的梳理总结，希望可以帮助大家对Stream有个更全面的认知，也可以更加高效的应用到项目开发中去。\nStream初相识概括讲，可以将Stream流操作分为3种类型：\n\n创建Stream\nStream中间处理\n终止Steam\n\n\n每个Stream管道操作类型都包含若干API方法，先列举下各个API方法的功能介绍。\n\n开始管道\n\n主要负责新建一个Stream流，或者基于现有的数组、List、Set、Map等集合类型对象创建出新的Stream流。\n\n\n\nAPI\n功能说明\n\n\n\nstream()\n创建出一个新的stream串行流对象\n\n\nparallelStream()\n创建出一个可并行执行的stream流对象\n\n\nStream.of()\n通过给定的一系列元素创建一个新的Stream串行流对象\n\n\n\n\n中间管道\n\n负责对Stream进行处理操作，并返回一个新的Stream对象，中间管道操作可以进行叠加。\n\n\n\nAPI\n功能说明\n\n\n\nfilter()\n按照条件过滤符合要求的元素， 返回新的stream流\n\n\nmap()\n将已有元素转换为另一个对象类型，一对一逻辑，返回新的stream流\n\n\nflatMap()\n将已有元素转换为另一个对象类型，一对多逻辑，即原来一个元素对象可能会转换为1个或者多个新类型的元素，返回新的stream流\n\n\nlimit()\n仅保留集合前面指定个数的元素，返回新的stream流\n\n\nskip()\n跳过集合前面指定个数的元素，返回新的stream流\n\n\nconcat()\n将两个流的数据合并起来为1个新的流，返回新的stream流\n\n\ndistinct()\n对Stream中所有元素进行去重，返回新的stream流\n\n\nsorted()\n对stream中所有的元素按照指定规则进行排序，返回新的stream流\n\n\npeek()\n对stream流中的每个元素进行逐个遍历处理，返回处理后的stream流\n\n\n\n\n终止管道\n\n顾名思义，通过终止管道操作之后，Stream流将会结束，最后可能会执行某些逻辑处理，或者是按照要求返回某些执行后的结果数据。\n\n\n\nAPI\n功能说明\n\n\n\ncount()\n返回stream处理后最终的元素个数\n\n\nmax()\n返回stream处理后的元素最大值\n\n\nmin()\n返回stream处理后的元素最小值\n\n\nfindFirst()\n找到第一个符合条件的元素时则终止流处理\n\n\nfindAny()\n找到任何一个符合条件的元素时则退出流处理，这个对于串行流时与findFirst相同，对于并行流时比较高效，任何分片中找到都会终止后续计算逻辑\n\n\nanyMatch()\n返回一个boolean值，类似于isContains(),用于判断是否有符合条件的元素\n\n\nallMatch()\n返回一个boolean值，用于判断是否所有元素都符合条件\n\n\nnoneMatch()\n返回一个boolean值， 用于判断是否所有元素都不符合条件\n\n\ncollect()\n将流转换为指定的类型，通过Collectors进行指定\n\n\ntoArray()\n将流转换为数组\n\n\niterator()\n将流转换为Iterator对象\n\n\nforeach()\n无返回值，对元素进行逐个遍历，然后执行给定的处理逻辑\n\n\nStream方法使用map与flatMapmap与 flatMap都是用于转换已有的元素为其它元素，区别点在于：\n\nmap 必须是一对一的，即每个元素都只能转换为1个新的元素\nflatMap 可以是一对多的，即每个元素都可以转换为1个或者多个新的元素\n\n\n比如：有一个字符串ID列表，现在需要将其转为User对象列表。可以使用map来实现：\n\n&#x2F;**\n * 演示map的用途：一对一转换\n *&#x2F;\npublic void stringToIntMap() &#123;\n    List&lt;String&gt; ids &#x3D; Arrays.asList(&quot;205&quot;, &quot;105&quot;, &quot;308&quot;, &quot;469&quot;, &quot;627&quot;, &quot;193&quot;, &quot;111&quot;);\n    &#x2F;&#x2F; 使用流操作\n    List&lt;User&gt; results &#x3D; ids.stream()\n            .map(id -&gt; &#123;\n                User user &#x3D; new User();\n                user.setId(id);\n                return user;\n            &#125;)\n            .collect(Collectors.toList());\n    System.out.println(results);\n&#125;\n\n\n执行之后，会发现每一个元素都被转换为对应新的元素，但是前后总元素个数是一致的：\n\n[User&#123;id&#x3D;&#39;205&#39;&#125;, \n User&#123;id&#x3D;&#39;105&#39;&#125;,\n User&#123;id&#x3D;&#39;308&#39;&#125;, \n User&#123;id&#x3D;&#39;469&#39;&#125;, \n User&#123;id&#x3D;&#39;627&#39;&#125;, \n User&#123;id&#x3D;&#39;193&#39;&#125;, \n User&#123;id&#x3D;&#39;111&#39;&#125;]\n\n\n\n再比如：现有一个句子列表，需要将句子中每个单词都提取出来得到一个所有单词列表。这种情况用map就搞不定了，需要 flatMap上场了：\n\npublic void stringToIntFlatmap() &#123;\n    List&lt;String&gt; sentences &#x3D; Arrays.asList(&quot;hello world&quot;,&quot;Jia Gou Wu Dao&quot;);\n    &#x2F;&#x2F; 使用流操作\n    List&lt;String&gt; results &#x3D; sentences.stream()\n            .flatMap(sentence -&gt; Arrays.stream(sentence.split(&quot; &quot;)))\n            .collect(Collectors.toList());\n    System.out.println(results);\n&#125;\n\n\n执行结果如下，可以看到结果列表中元素个数是比原始列表元素个数要多的：\n\n[hello, world, Jia, Gou, Wu, Dao]\n\n\n这里需要补充一句，flatMap操作的时候其实是先每个元素处理并返回一个新的Stream，然后将多个Stream展开合并为了一个完整的新的Stream，如下：\n\npeek和foreach方法peek和 foreach，都可以用于对元素进行遍历然后逐个的进行处理。\n但根据前面的介绍，peek属于中间方法，而foreach属于终止方法。这也就意味着peek只能作为管道中途的一个处理步骤，而没法直接执行得到结果，其后面必须还要有其它终止操作的时候才会被执行；而foreach作为无返回值的终止方法，则可以直接执行相关操作。\n\npublic void testPeekAndforeach() &#123;\n    List&lt;String&gt; sentences &#x3D; Arrays.asList(&quot;hello world&quot;,&quot;Jia Gou Wu Dao&quot;);\n    &#x2F;&#x2F; 演示点1： 仅peek操作，最终不会执行\n    System.out.println(&quot;----before peek----&quot;);\n    sentences.stream().peek(sentence -&gt; System.out.println(sentence));\n    System.out.println(&quot;----after peek----&quot;);\n    &#x2F;&#x2F; 演示点2： 仅foreach操作，最终会执行\n    System.out.println(&quot;----before foreach----&quot;);\n    sentences.stream().forEach(sentence -&gt; System.out.println(sentence));\n    System.out.println(&quot;----after foreach----&quot;);\n    &#x2F;&#x2F; 演示点3： peek操作后面增加终止操作，peek会执行\n    System.out.println(&quot;----before peek and count----&quot;);\n    sentences.stream().peek(sentence -&gt; System.out.println(sentence)).count();\n    System.out.println(&quot;----after peek and count----&quot;);\n&#125;\n\n\n输出结果可以看出，peek独自调用时并没有被执行、但peek后面加上终止操作之后便可以被执行，而foreach可以直接被执行：\n\n----before peek----\n----after peek----\n----before foreach----\nhello world\nJia Gou Wu Dao\n----after foreach----\n----before peek and count----\nhello world\nJia Gou Wu Dao\n----after peek and count----\n\n\n\n\nfilter、sorted、distinct、limit这几个都是常用的Stream的中间操作方法，具体的方法的含义在上面的表格里面有说明。具体使用的时候，可以根据需要选择一个或者多个进行组合使用，或者同时使用多个相同方法的组合：\n\npublic void testGetTargetUsers() &#123;\n    List&lt;String&gt; ids &#x3D; Arrays.asList(&quot;205&quot;,&quot;10&quot;,&quot;308&quot;,&quot;49&quot;,&quot;627&quot;,&quot;193&quot;,&quot;111&quot;, &quot;193&quot;);\n    &#x2F;&#x2F; 使用流操作\n    List&lt;Dept&gt; results &#x3D; ids.stream()\n            .filter(s -&gt; s.length() &gt; 2)\n            .distinct()\n            .map(Integer::valueOf)\n            .sorted(Comparator.comparingInt(o -&gt; o))\n            .limit(3)\n            .map(id -&gt; new Dept(id))\n            .collect(Collectors.toList());\n    System.out.println(results);\n&#125;\n\n\n上面的代码片段的处理逻辑很清晰：\n\n使用filter过滤掉不符合条件的数据\n通过distinct对存量元素进行去重操作\n通过map操作将字符串转成整数类型\n借助sorted指定按照数字大小正序排列\n使用limit截取排在前3位的元素\n又一次使用map将id转为Dept对象类型\n使用collect终止操作将最终处理后的数据收集到list中\n\n输出结果：\n[Dept&#123;id&#x3D;111&#125;,  Dept&#123;id&#x3D;193&#125;,  Dept&#123;id&#x3D;205&#125;]\n\n\n\n简单结果终止方法按照前面介绍的，终止方法里面像 count、max、min、findAny、findFirst、anyMatch、allMatch、noneMatch等方法，均属于这里说的简单结果终止方法。所谓简单，指的是其结果形式是数字、布尔值或者Optional对象值等。\n\npublic void testSimpleStopOptions() &#123;\n    List&lt;String&gt; ids &#x3D; Arrays.asList(&quot;205&quot;, &quot;10&quot;, &quot;308&quot;, &quot;49&quot;, &quot;627&quot;, &quot;193&quot;, &quot;111&quot;, &quot;193&quot;);\n    &#x2F;&#x2F; 统计stream操作后剩余的元素个数\n    System.out.println(ids.stream().filter(s -&gt; s.length() &gt; 2).count());\n    &#x2F;&#x2F; 判断是否有元素值等于205\n    System.out.println(ids.stream().filter(s -&gt; s.length() &gt; 2).anyMatch(&quot;205&quot;::equals));\n    &#x2F;&#x2F; findFirst操作\n    ids.stream().filter(s -&gt; s.length() &gt; 2)\n            .findFirst()\n            .ifPresent(s -&gt; System.out.println(&quot;findFirst:&quot; + s));\n&#125;\n\n\n执行后结果为：\n\n6\ntrue\nfindFirst:205\n\n\n\n避坑提醒\n这里需要补充提醒下，一旦一个Stream被执行了终止操作之后，后续便不可以再读这个流执行其他的操作了，否则会报错，看下面示例：\n\npublic void testHandleStreamAfterClosed() &#123;\n    List&lt;String&gt; ids &#x3D; Arrays.asList(&quot;205&quot;, &quot;10&quot;, &quot;308&quot;, &quot;49&quot;, &quot;627&quot;, &quot;193&quot;, &quot;111&quot;, &quot;193&quot;);\n    Stream&lt;String&gt; stream &#x3D; ids.stream().filter(s -&gt; s.length() &gt; 2);\n    &#x2F;&#x2F; 统计stream操作后剩余的元素个数\n    System.out.println(stream.count());\n    System.out.println(&quot;-----下面会报错-----&quot;);\n    &#x2F;&#x2F; 判断是否有元素值等于205\n    try &#123;\n        System.out.println(stream.anyMatch(&quot;205&quot;::equals));\n    &#125; catch (Exception e) &#123;\n        e.printStackTrace();\n    &#125;\n    System.out.println(&quot;-----上面会报错-----&quot;);\n&#125;\n\n\n执行的时候，结果如下：\n\n6\n-----下面会报错-----\njava.lang.IllegalStateException: stream has already been operated upon or closed\n\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:229)\n\tat java.util.stream.ReferencePipeline.anyMatch(ReferencePipeline.java:449)\n\tat com.veezean.skills.stream.StreamService.testHandleStreamAfterClosed(StreamService.java:153)\n\tat com.veezean.skills.stream.StreamService.main(StreamService.java:176)\n-----上面会报错-----\n\n\n因为stream已经被执行 count()终止方法了，所以对stream再执行 anyMatch方法的时候，就会报错 stream has already been operated upon or closed，这一点在使用的时候需要特别注意。\n\n结果收集终止方法因为Stream主要用于对集合数据的处理场景，所以除了上面几种获取简单结果的终止方法之外，更多的场景是获取一个集合类的结果对象，比如List、Set或者HashMap等。\n这里就需要 collect方法出场了，它可以支持生成如下类型的结果数据：\n\n一个 集合类，比如List、Set或者HashMap等\nStringBuilder对象，支持将多个 字符串进行拼接处理并输出拼接后结果\n一个可以记录个数或者计算总和的对象（数据批量运算统计）\n\n\n生成集合应该算是collect最常被使用到的一个场景了：\n\npublic void testCollectStopOptions() &#123;\n    List&lt;Dept&gt; ids &#x3D; Arrays.asList(new Dept(17), new Dept(22), new Dept(23));\n    &#x2F;&#x2F; collect成list\n    List&lt;Dept&gt; collectList &#x3D; ids.stream().filter(dept -&gt; dept.getId() &gt; 20)\n            .collect(Collectors.toList());\n    System.out.println(&quot;collectList:&quot; + collectList);\n    &#x2F;&#x2F; collect成Set\n    Set&lt;Dept&gt; collectSet &#x3D; ids.stream().filter(dept -&gt; dept.getId() &gt; 20)\n            .collect(Collectors.toSet());\n    System.out.println(&quot;collectSet:&quot; + collectSet);\n    &#x2F;&#x2F; collect成HashMap，key为id，value为Dept对象\n    Map&lt;Integer, Dept&gt; collectMap &#x3D; ids.stream().filter(dept -&gt; dept.getId() &gt; 20)\n            .collect(Collectors.toMap(Dept::getId, dept -&gt; dept));\n    System.out.println(&quot;collectMap:&quot; + collectMap);\n&#125;\n\n\n结果如下：\n\ncollectList:[Dept&#123;id&#x3D;22&#125;, Dept&#123;id&#x3D;23&#125;]\ncollectSet:[Dept&#123;id&#x3D;23&#125;, Dept&#123;id&#x3D;22&#125;]\ncollectMap:&#123;22&#x3D;Dept&#123;id&#x3D;22&#125;, 23&#x3D;Dept&#123;id&#x3D;23&#125;&#125;\n\n\n\n生成拼接字符串将一个List或者数组中的值拼接到一个字符串里并以逗号分隔开，这个场景相信大家都不陌生吧？\n如果通过 for循环和 StringBuilder去循环拼接，还得考虑下最后一个逗号如何处理的问题，很繁琐:\n\npublic void testForJoinStrings() &#123;\n    List&lt;String&gt; ids &#x3D; Arrays.asList(&quot;205&quot;, &quot;10&quot;, &quot;308&quot;, &quot;49&quot;, &quot;627&quot;, &quot;193&quot;, &quot;111&quot;, &quot;193&quot;);\n    StringBuilder builder &#x3D; new StringBuilder();\n    for (String id : ids) &#123;\n        builder.append(id).append(&#39;,&#39;);\n    &#125;\n    &#x2F;&#x2F; 去掉末尾多拼接的逗号\n    builder.deleteCharAt(builder.length() - 1);\n    System.out.println(&quot;拼接后：&quot; + builder.toString());\n&#125;\n\n\n但是现在有了Stream，使用 collect可以轻而易举的实现：\n\npublic void testCollectJoinStrings() &#123;\n    List&lt;String&gt; ids &#x3D; Arrays.asList(&quot;205&quot;, &quot;10&quot;, &quot;308&quot;, &quot;49&quot;, &quot;627&quot;, &quot;193&quot;, &quot;111&quot;, &quot;193&quot;);\n    String joinResult &#x3D; ids.stream().collect(Collectors.joining(&quot;,&quot;));\n    System.out.println(&quot;拼接后：&quot; + joinResult);\n&#125;\n\n\n两种方式都可以得到完全相同的结果，但Stream的方式更优雅：\n拼接后：205,10,308,49,627,193,111,193\n\n\n📢 敲黑板：\n关于这里的说明，评论区中很多的小伙伴提出过疑问，就是这个场景其实使用 String.join() 就可以搞定了，并不需要上面使用 stream 的方式去实现。这里要声明下，Stream的魅力之处就在于其可以结合到其它的业务逻辑中进行处理，让代码逻辑更加的自然、一气呵成。如果纯粹是个String字符串拼接的诉求，确实没有必要使用Stream来实现，毕竟杀鸡焉用牛刀嘛~ 但是可以看看下面给出的这个示例，便可以感受出使用Stream进行字符串拼接的真正魅力所在。\n\n\n数据批量数学运算还有一种场景，实际使用的时候可能会比较少，就是使用collect生成数字数据的总和信息，也可以了解下实现方式：\n\npublic void testNumberCalculate() &#123;\n    List&lt;Integer&gt; ids &#x3D; Arrays.asList(10, 20, 30, 40, 50);\n    &#x2F;&#x2F; 计算平均值\n    Double average &#x3D; ids.stream().collect(Collectors.averagingInt(value -&gt; value));\n    System.out.println(&quot;平均值：&quot; + average);\n    &#x2F;&#x2F; 数据统计信息\n    IntSummaryStatistics summary &#x3D; ids.stream().collect(Collectors.summarizingInt(value -&gt; value));\n    System.out.println(&quot;数据统计信息： &quot; + summary);\n&#125;\n\n\n上面的例子中，使用collect方法来对list中元素值进行数学运算，结果如下：\n\n平均值：30.0\n总和： IntSummaryStatistics&#123;count&#x3D;5, sum&#x3D;150, min&#x3D;10, average&#x3D;30.000000, max&#x3D;50&#125;\n\n\n\n并行Stream机制说明使用并行流，可以有效利用计算机的多CPU硬件，提升逻辑的执行速度。并行流通过将一整个stream划分为 多个片段，然后对各个分片流并行执行处理逻辑，最后将各个分片流的执行结果汇总为一个整体流。\n\n约束与限制并行流类似于多线程在并行处理，所以与多线程场景相关的一些问题同样会存在，比如死锁等问题，所以在并行流终止执行的函数逻辑，必须要保证线程安全。\n\n回答最初的问题到这里，关于JAVA Stream的相关概念与用法介绍，基本就讲完了。我们再把焦点切回本文刚开始时提及的一个问题：\nStream相较于传统的foreach的方式处理stream，到底有啥优势？\n根据前面的介绍，我们应该可以得出如下几点答案：\n\n代码更简洁、偏声明式的编码风格，更容易体现出代码的逻辑意图\n逻辑间解耦，一个stream中间处理逻辑，无需关注上游与下游的内容，只需要按约定实现自身逻辑即可\n并行流场景效率会比迭代器逐个循环更高\n函数式接口，延迟执行的特性，中间管道操作不管有多少步骤都不会立即执行，只有遇到终止操作的时候才会开始执行，可以避免一些中间不必要的操作消耗\n\n当然了，Stream也不全是优点，在有些方面也有其弊端：\n\n代码调测debug不便\n程序员从历史写法切换到Stream时，需要一定的适应时间\n\n\n总结好啦，关于JAVA Stream的理解要点与使用技能的阐述就先到这里啦。那通过上面的介绍，各位小伙伴们是否已经跃跃欲试了呢？快去项目中使用体验下吧！当然啦，如果有疑问，也欢迎找我一起探讨探讨咯。\n此外：\n\n关于Stream中collect的分组、分片等进阶操作，以及对并行流的深入探讨，因为涉及内容比较多且相对独立，我会在后续的文档中展开专门介绍下，如果有兴趣的话，可以点个关注、避免迷路。\n关于本文中涉及的演示代码的完整示例，我已经整理并提交到github中，如果您有需要，可以自取：https://github.com/veezean/JavaBasicSkills\n\n\n我是悟道，聊技术、又不仅仅聊技术~\n如果觉得有用，请点个关注，也可以关注下我的公众号【架构悟道】，获取更及时的更新。\n期待与你一起探讨，一起成长为更好的自己。\n\n","slug":"2021/hello-world4","date":"2023-01-27T10:09:10.448Z","categories_index":"缓存实践专栏6","tags_index":"","author_index":"Veezean"},{"id":"2e3da9cee4e351233c8d55f8ea0c8423","title":"testgagagaga","content":"","slug":"testgagagaga-1","date":"2023-01-20T06:25:49.000Z","categories_index":"","tags_index":"","author_index":"Veezean"},{"id":"2e3da9cee4e351233c8d55f8ea0c8423","title":"testgagagaga","content":"","slug":"testgagagaga","date":"2023-01-20T06:25:15.000Z","categories_index":"","tags_index":"","author_index":"Veezean"},{"id":"b9663f58f18133b35bfe243f3e916a80","title":"Hello World","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n","slug":"2022/hello-world","date":"2023-01-14T16:00:00.000Z","categories_index":"缓存实践专栏3","tags_index":"缓存,JAVA,高并发,高负载","author_index":"Veezean"},{"id":"53f85b32e6814fbf1fd251f6dad13a7e","title":"sdfsdfsdfsdfsdfsd","content":"\n大家好，又见面了。\n\n\n\n\n\n\n\n\n\n\n本文是笔者作为掘金技术社区签约作者的身份输出的缓存专栏系列内容，将会通过系列专题，讲清楚缓存的方方面面。如果感兴趣，欢迎关注以获取后续更新。\n\n在服务端开发中，缓存常常被当做系统性能扛压的不二之选。在实施方案上，缓存使用策略虽有一定普适性，却也并非完全绝对，需要结合实际的项目诉求与场景进行综合权衡与考量，进而得出符合自己项目的最佳实践。\n缓存使用的演进现有这么一个系统：\n\n\n\n\n\n\n\n\n\n一个互动论坛系统，用户登录系统之后，可以在论坛上查看帖子列表、查看帖子详情、发表帖子、评论帖子、为帖子点赞等操作。\n系统中所有的配置数据与业务数据均存储在数据库中。随着业务的发展，注册用户量越来越多，然后整个系统的响应速度也越来越慢，用户体验越来越差，用户逐渐出现流失。\n本地缓存的牛刀小试为了挽救这一局面，开发人员需要介入去分析性能瓶颈并尝试优化提升响应速度，并很快找到响应慢的瓶颈在数据库的频繁操作，于是想到了使用缓存来解决问题。\n于是，开发人员在项目中使用了基于接口维度的短期缓存，对每个接口的请求参数（帖子ID）与响应内容缓存一定的时间（比如1分钟），对于相同的请求，如果匹配到缓存则直接返回缓存的结果即可，不用再次去执行查询数据库以及业务维度的运算逻辑。\n\nJAVA中有很多的开源框架都有提供类似的能力支持，比如Ehcache或者Guava Cache、Caffeine Cache等，可以通过简单的添加注解的方式就实现上述需要的缓存效果。比如使用Ehcache来实现接口接口缓存的时候，代码使用方式如下（这里先简单的演示下，后续的系列文档中会专门对这些框架进行深入的探讨）：\n@Cacheable(value&#x3D;&quot;UserDetailCache&quot;, key&#x3D;&quot;#userId&quot;)\npublic UserDetail queryUserDetailById(String userId) &#123;\n    UserEntity userEntity &#x3D; userMapper.queryByUserId(userId);\n    return convertEntityToUserDetail(userEntity);\n&#125;\n\n基上面的本地缓存策略改动后重新上线，整体的响应性能上果然提升了很多。本地缓存的策略虽然有效地提升了处理请求的速度，但新的问题也随之浮现。有用户反馈，社区内的帖子列表多次刷新后会出现内容不一致的情况，有的帖子刷新之后会从列表消失，多次刷新后偶尔会出现。\n其实这就是本地缓存在集群多节点场景下会遇到的一个很常见的缓存漂移现象：\n\n因为业务集群存在多个节点，而缓存是每个业务节点本地独立构建的，所以才出现了更新场景导致的本地缓存不一致的问题，进而表现为上述问题现象。\n集中式缓存的初露锋芒为了解决集群内多个节点间执行写操作之后，各节点本地缓存不一致的问题，开发人员想到可以构建一个集中式缓存，然后所有业务节点都读取或者更新同一份缓存数据，这样就可以完美地解决节点间缓存不一致的问题了。\n\n业界成熟的集中式缓存有很多，最出名的莫过于很多人都耳熟能详的Redis，或者是在各种面试中常常被拿来与Redis进行比较的Memcached。也正是由于它们出色的自身性能表现，在当前的各种分布式系统中，Redis近乎已经成为了一种标配，常常与MySQL等持久化数据库搭配使用，放在数据库前面进行扛压。比如下面图中示例的一种最简化版本的组网架构：\n\n开发人员对缓存进行了整改，将本地缓存改为了Redis集中式缓存。这样一来：\n\n缓存不一致问题解决：解决了各个节点间数据不一致的问题。\n\n单机内存容量限制解决：使用了Redis这种分布式的集中式缓存，扩大了内存缓存的容量范围，可以顺便将很多业务层面的数据全部加载到Redis中分片进行缓存，性能也相比而言得到了提升。\n\n\n似乎使用集中式缓存已经是分布式系统中的最优解了，但是现实情况真的就这么简单么？也不尽然！\n多级缓存的珠联璧合在尝到了集中式缓存的甜头之后，暖心的程序员们想到要彻底为数据库减压，将所有业务中需要频繁使用的数据全部同步存储到Redis中，然后业务使用的时候直接从Redis中获取相关数据，大大地减少了数据库的请求频次。但是改完上线之后，发现有些处理流程中并没有太大的性能提升。缘何如此？只因为对集中式缓存的过分滥用！分析发现这些流程的处理需要涉及大量的交互与数据整合逻辑，一个流程需要访问近乎30次Redis！虽然Redis的单次请求处理性能极高，甚至可以达到微秒级别的响应速度，但是每个流程里面几十次的网络IO交互，导致频繁的IO请求，以及线程的阻塞与唤醒切换交替，使得系统在线程上下文切换层面浪费巨大。\n那么，要想破局，最常规的手段便是尝试降低对集中式缓存（如Redis）的请求数量，降低网络IO交互次数。而如何来降低呢？ —— 又回到了本地缓存！集中式缓存并非是分布式系统中提升性能的银弹，但我们可以将本地缓存与集中式缓存结合起来使用，取长补短，实现效果最大化。如图所示：\n\n上图演示的也即多级缓存的策略。具体而言：\n\n对于一些变更频率比较高的数据，采用集中式缓存，这样可以确保数据变更之后所有节点都可以实时感知到，确保数据一致；\n\n对于一些极少变更的数据（比如一些系统配置项）或者是一些对短期一致性要求不高的数据（比如用户昵称、签名等）则采用本地缓存，大大减少对远端集中式缓存的网络IO次数。\n\n\n这样一来，系统的响应性能又得到了进一步的提升。\n通过对缓存使用策略的一步步演进，我们可以感受到缓存的恰当使用对系统性能的帮助作用。\n\n无处不在的缓存缓存存在的初衷，就是为了兼容两个处理速度不一致的场景对接适配的。在我们的日常生活中，也常常可以看到“缓存”的影子。比如对于几年前比较盛行的那种带桶的净水器（见下图），由于净水的功率比较小，导致实时过滤得到纯净水的水流特别的缓慢，用户倒一杯水要等2分钟，体验太差，所以配了个蓄水桶，净水机先慢慢的将净化后的水存储到桶中，然后用户倒水的时候可以从桶里快速的倒出，无需焦急等待 —— 这个蓄水桶，便是一个缓存器。\n\n编码源于生活，CPU的高速缓存设计就是这一生活实践在计算机领域的原样复制。缓存可以说在软件世界里无处不在，除了我们自己的业务系统外，在网络传输、操作系统、中间件、基础框架中都可以看到缓存的影子。如：\n\n网络传输场景。\n\n比如ARP协议，基于ARP缓存表进行IP与终端硬件MAC地址之间的缓存映射。这样与对端主机之间有通信需求的时候，就可以在ARP缓存中查找到IP对应的对端设备MAC地址，避免每次请求都需要去发送ARP请求查询MAC地址。\n\nMyBatis的多级缓存。\n\nMyBatis作为JAVA体系中被广泛使用的数据库操作框架，其内部为了提升处理效率，构建了一级缓存与二级缓存，大大减少了对SQL的重复执行次数。\n\nCPU中的缓存。\n\nCPU与内存之间有个临时存储器（高速缓存），容量虽比内存小，但是处理速度却远快于普通内存。高速缓存的机制，有效地解决了CPU运算速度与内存读写速度不匹配的问题。\n\n缓存的使用场景缓存作为互联网类软件系统架构与实现中的基石般的存在，不仅仅是在系统扛压或者接口处理速度提升等性能优化方案，在其他多个方面都可以发挥其独一无二的关键价值。下面就让我们一起来看看缓存都可以用在哪些场景上，可以解决我们哪方面的痛点。\n降低自身CPU消耗如前面章节中提到的项目实例，缓存最典型的使用场景就是用在系统的性能优化上。而在性能优化层面，一个经典的策略就是“空间换时间”。比如：\n\n在数据库表中做一些字段冗备。\n\n比如用户表T_User和部门表T_Department，在T_User表中除了有个Department_Id字段与T_Department表进行关联之外，还额外在T_User表中存储Department_Name值。这样在很多需要展示用户所属部门信息的时候就省去了多表关联查询的操作。\n\n\n对一些中间处理结果进行存储。\n\n比如系统中的数据报表模块，需要对整个系统内所有的关联业务数据进行计算统计，且需要多张表多来源数据之间的综合汇总之后才能得到最终的结果，整个过程的计算非常的耗时。如果借助缓存，则可以将一些中间计算结果进行暂存，然后报表请求中基于中间结果进行二次简单处理即可。这样可以大大降低基于请求触发的实时计算量。\n在“空间换时间”实施策略中，缓存是该策略的核心、也是被使用的最为广泛的一种方案。借助缓存，可以将一些CPU耗时计算的处理结果进行缓存复用，以降低重复计算工作量，达到降低CPU占用的效果。\n\n减少对外IO交互上面介绍的使用缓存是为了不断降低请求处理时对自身CPU占用，进而提升服务的处理性能。这里我们介绍缓存的另一典型使用场景，就是减少系统对外依赖的请求频次。即通过将一些从远端请求回来的响应结果进行缓存，后面直接使用此缓存结果而无需再次发起网络IO请求交互。\n对于服务端而言，通过构建缓存的方式来减少自身对外的IO请求，主要有几个考量出发点：\n\n从自身性能层面考虑，减少对外IO操作，降低了对外接口的响应时延，也对服务端自身处理性能有一定提升。\n\n从对端服务稳定性层面考虑，避免对端服务负载过大。很多时候调用方和被调用方系统的承压能力是不匹配的，甚至有些被调用方系统可能是不承压的。为了避免将对端服务压垮，需要调用方缓存请求结果，降低IO请求。\n\n从自身可靠性层面而言，将一些远端服务请求到的结果缓存起来，即使远端服务出现故障，自身业务依旧可以基于缓存数据进行正常业务处理，起到一个兜底作用，提升自身的抗风险能力。\n\n\n在分布式系统服务治理范畴内，服务注册管理服务是必不可少的，比如SpringCloud家族的Eureka，或者是Alibaba开源的Nacos。它们对于缓存的利用，可以说是对上面所提几点的完美阐述。\n以Nacos为例：\n\n除了上述的因素之外，对一些移动端APP或者H5界面而言，缓存的使用还有一个层面的考虑，即降低用户的流量消耗，通过将一些资源类数据缓存到本地，避免反复去下载，给用户省点流量，也可以提升用户的使用体验（界面渲染速度快，减少出现白屏等待的情况）。\n\n提升用户个性化体验缓存除了在系统性能提升或系统可靠性兜底等场景发挥价值外，在APP或者web类用户侧产品中，还经常被用于存储一些临时非永久的个性化使用习惯配置或者身份数据，以提升用户的个性化使用体验。\n\n缓存cookie、session等身份鉴权信息，这样就可以避免用户每次访问都需要进行身份验证。\n\n\n\n记住一些用户上次操作习惯，比如用户在一个页面上将列表分页查询设置为100条&#x2F;页，则后续在系统内访问其它列表页面时，都沿用这一设置。\n\n缓存用户的一些本地设置，这个主要是APP端常用的功能，可以在缓存中保存些与当前设备绑定的设置信息，仅对当前设备有效。比如同一个账号登录某个APP，用户希望在手机端可以显示深色主题，而PAD端则显示浅色主体，这种基于设备的个性化设置，可以缓存到设备本身即可。\n\n\n\n业务与缓存的集成模式如前所述，我们可以在不同的方面使用缓存来辅助达成项目在某些方面的诉求。而根据使用场景的不同，在结合缓存进行业务逻辑实现的时候，也会存在不同的架构模式，典型的会有旁路型缓存、穿透型缓存与异步型缓存三种。\n旁路型缓存在旁路型缓存模式中，业务自行负责与缓存以及数据库之间的交互，可以自由决定缓存未命中场景的处理策略，更加契合大部分业务场景的定制化诉求。\n\n由于业务模块自行实现缓存与数据库之间的数据写入与更新的逻辑，实际实现的时候需要注意下在高并发场景的数据一致性问题，以及可能会出现的缓存击穿、缓存穿透、缓存雪崩等问题的防护。\n旁路型缓存是实际业务中最常使用的一种架构模式，在后面的内容中，我们还会不断的涉及到旁路缓存中相关的内容。\n\n穿透型缓存穿透型缓存在实际业务中使用的较少，主要是应用在一些缓存类的中间件中，或者在一些大型系统中专门的数据管理模块中使用。\n一般情况下，业务使用缓存的时候，会是先尝试读取缓存，在尝试读取DB，而使用穿透型缓存架构时，会有专门模块将这些动作封装成黑盒的，业务模块不会与数据库进行直接交互。如下图所示：\n\n这种模式对业务而言是比较友好的，业务只需调用缓存接口即可，无需自行实现缓存与DB之间的交互策略。\n异步型缓存还有一种缓存的使用模式，可以看作是穿透型缓存的演进异化版本，其使用场景也相对较少，即异步型缓存。其主要策略就是业务侧请求的实时读写交互都是基于缓存进行，任何数据的读写也完全基于缓存进行操作。此外，单独实现一个数据持久化操作(独立线程或者进程中执行)，用于将缓存中变更的数据写入到数据库中。\n\n这种情况，实时业务读写请求完全基于缓存进行，而将数据库仅仅作为一个数据持久化存储的备份盘。由于实时业务请求仅与缓存进行交互，所以在性能上可以得到更好的表现。但是这种模式也存在一个致命的问题：数据可靠性！因为是异步操作，所以在下一次数据写入DB前，会有一段时间数据仅存在于缓存中，一旦缓存服务宕机，这部分数据将会丢失。所以这种模式仅适用于对数据一致性要求不是特别高的场景。\n缓存的优秀实践缓存与持久化存储的一个很大的不同点就是缓存的定位应该是一种辅助角色，是一种锦上添花般的存在。\n缓存也是一把双刃剑，基于缓存可以大幅提升我们的系统并发与承压能力，但稍不留神也可能会让我们的系统陷入灭顶之灾。所以我们在决定使用缓存的时候，需要知晓缓存设计与使用的一些关键要点，才可以让我们在使用的时候更加游刃有余。\n可删除重建可删除重建，这是缓存与持久化存储最大的一个差别。缓存的定位一定是为了辅助业务处理而生的，也就是说缓存有则使用，没有也不会影响到我们具体的业务运转。此外，即使我们的缓存数据除了问题，我们也可以将其删除重建。\n这一点在APP类的产品中体现的会比较明显。比如对于微信APP的缓存，就有明确的提示说缓存可以删除而不会影响其功能使用：\n\n同样地，我们也可以去放心的清理浏览器的缓存，而不用担心清理之后我们浏览器或者网页的功能会出现异常（最多就是需要重新下载或者重建缓存数据，速度会有一些慢）。\n\n相同的逻辑，在服务端构建的一些缓存，也应该具备此特性。比如基于内存的缓存，当业务进程重启后，应该有途径可以将缓存重建出来（比如从MySQL中加载数据然后构建缓存，或者是缓存从0开始基于请求触发而构建）。\n\n有兜底屏障缓存作为高并发类系统中的核心组件，负责抗住大部分的并发请求，一旦缓存组件出问题，往往对整个系统会造成毁灭性的打击。所以我们的缓存在实现的时候必须要有充足且完备的兜底与自恢复机制。需要做到以下几点：\n\n关注下缓存数据量超出承受范围的处理策略，比如定好数据的淘汰机制。\n\n避免缓存集中失效，比如批量加载数据到缓存的时候随机打散过期时间，避免同一时间大批量缓存失效引发缓存雪崩问题。\n\n有效地冷数据预热加载机制，以及热点数据防过期机制，避免出现大量对冷数据的请求无法命中缓存或者热点数据突然失效，导致缓存击穿问题。\n\n合理的防身自保手段，比如采用布隆过滤器机制，避免被恶意请求攻陷，导致缓存穿透类的问题。\n\n\n缓存的可靠性与兜底策略设计，是一个宏大且宽泛的命题，在本系列专栏后续的文章中，我们会逐个深入的探讨。\n\n关注缓存的一致性保证在高并发类的系统中进行数据更新的时候，缓存与数据库的数据一致性问题，是一个永远无法绕过的话题。对于基于旁路型缓存的大部分业务而言，数据更新操作，一般可以组合出几种不同的处理策略：\n\n先更新缓存，再更新数据库\n\n先更新数据库， 再更新缓存\n\n先删除缓存，再更新数据库\n\n先更新数据库，再删除缓存\n\n\n由于大部分数据库都支持事务，而几乎所有的缓存操作都不具有事务性。所以在一些写操作并发不是特别高且一致性要求不是特别强烈的情况下，可以简单的借助数据库的事务进行控制。比如先更新数据库再更新缓存，如果缓存更新失败则回滚数据库事务。\n然而在一些并发请求特别高的时候，基于事务控制来保证数据一致性往往会对性能造成影响，且事务隔离级别设置的越高影响越大，所以也可以采用一些其它辅助策略，来替代事务的控制，如重试机制、或异步补偿机制、或多者结合方式等。\n比如下图所示的这种策略：\n\n上图的数据更新处理策略，可以有效地保证数据的最终一致性，降低极端情况可能出现数据不一致的概率，并兜底增加了数据不一致时的自恢复能力。\n数据一致性保证作为缓存的另一个重要命题，我们会在本系列专栏后续的文章中专门进行深入的剖析。\n\n总结回顾本篇文章的内容中，我们对缓存的各个方面进行了一个简单的阐述与了解，也可以看出缓存对于一个软件系统的重要价值。通过对缓存的合理、充分利用，可以大大的增强我们的系统承压性能、提升产品的用户体验。\n缓存作为高并发系统中的神兵利器被广泛使用，堪称高并发系统的基石之一。而缓存的内容还远远不止我们本篇文档中所介绍的这些、它是一个非常宏大的命题。\n\n为了能够将缓存的方方面面彻底的讲透、讲全，在接下来的一段时间里，我会以系列专栏的形式，从不同的角度对缓存的方方面面进行探讨。不仅仅着眼于如何去使用缓存、也一起聊聊缓存设计中的一些哲学理念 —— 这一点是我觉得更有价值的一点，因为这些理念对提升我们的软件架构认知、完善我们的软件设计思维有很大的指导与借鉴意义。\n所以，如果你有兴趣，欢迎关注本系列专栏（深入理解缓存原理与实战设计），我会以我一贯的行文风格，用最简单的语言讲透复杂的逻辑，期待一起切磋、共同成长。\n\n我是悟道，聊技术、又不仅仅聊技术~\n如果觉得有用，请点赞 + 关注让我感受到您的支持。也可以关注下我的公众号【架构悟道】，获取更及时的更新。\n期待与你一起探讨，一起成长为更好的自己。\n\n\n","slug":"2021/hello-worldrrff3","date":"2022-11-16T16:00:00.000Z","categories_index":"缓存实践专栏5","tags_index":"缓存,JAVA,高并发,高负载","author_index":"Veezean"},{"id":"7af923229c8359748f91fb0bb5dd4c72","title":"聊一聊作为高并发系统基石之一的缓存，会用很简单，用好才是技术活","content":"\n大家好，又见面了。\n\n\n\n\n\n\n\n\n\n\n本文是笔者作为掘金技术社区签约作者的身份输出的缓存专栏系列内容，将会通过系列专题，讲清楚缓存的方方面面。如果感兴趣，欢迎关注以获取后续更新。\n\n在服务端开发中，缓存常常被当做系统性能扛压的不二之选。在实施方案上，缓存使用策略虽有一定普适性，却也并非完全绝对，需要结合实际的项目诉求与场景进行综合权衡与考量，进而得出符合自己项目的最佳实践。\n缓存使用的演进现有这么一个系统：\n\n\n\n\n\n\n\n\n\n一个互动论坛系统，用户登录系统之后，可以在论坛上查看帖子列表、查看帖子详情、发表帖子、评论帖子、为帖子点赞等操作。\n系统中所有的配置数据与业务数据均存储在数据库中。随着业务的发展，注册用户量越来越多，然后整个系统的响应速度也越来越慢，用户体验越来越差，用户逐渐出现流失。\n本地缓存的牛刀小试为了挽救这一局面，开发人员需要介入去分析性能瓶颈并尝试优化提升响应速度，并很快找到响应慢的瓶颈在数据库的频繁操作，于是想到了使用缓存来解决问题。\n于是，开发人员在项目中使用了基于接口维度的短期缓存，对每个接口的请求参数（帖子ID）与响应内容缓存一定的时间（比如1分钟），对于相同的请求，如果匹配到缓存则直接返回缓存的结果即可，不用再次去执行查询数据库以及业务维度的运算逻辑。\n\nJAVA中有很多的开源框架都有提供类似的能力支持，比如Ehcache或者Guava Cache、Caffeine Cache等，可以通过简单的添加注解的方式就实现上述需要的缓存效果。比如使用Ehcache来实现接口接口缓存的时候，代码使用方式如下（这里先简单的演示下，后续的系列文档中会专门对这些框架进行深入的探讨）：\n@Cacheable(value&#x3D;&quot;UserDetailCache&quot;, key&#x3D;&quot;#userId&quot;)\npublic UserDetail queryUserDetailById(String userId) &#123;\n    UserEntity userEntity &#x3D; userMapper.queryByUserId(userId);\n    return convertEntityToUserDetail(userEntity);\n&#125;\n\n基上面的本地缓存策略改动后重新上线，整体的响应性能上果然提升了很多。本地缓存的策略虽然有效地提升了处理请求的速度，但新的问题也随之浮现。有用户反馈，社区内的帖子列表多次刷新后会出现内容不一致的情况，有的帖子刷新之后会从列表消失，多次刷新后偶尔会出现。\n其实这就是本地缓存在集群多节点场景下会遇到的一个很常见的缓存漂移现象：\n\n因为业务集群存在多个节点，而缓存是每个业务节点本地独立构建的，所以才出现了更新场景导致的本地缓存不一致的问题，进而表现为上述问题现象。\n集中式缓存的初露锋芒为了解决集群内多个节点间执行写操作之后，各节点本地缓存不一致的问题，开发人员想到可以构建一个集中式缓存，然后所有业务节点都读取或者更新同一份缓存数据，这样就可以完美地解决节点间缓存不一致的问题了。\n\n业界成熟的集中式缓存有很多，最出名的莫过于很多人都耳熟能详的Redis，或者是在各种面试中常常被拿来与Redis进行比较的Memcached。也正是由于它们出色的自身性能表现，在当前的各种分布式系统中，Redis近乎已经成为了一种标配，常常与MySQL等持久化数据库搭配使用，放在数据库前面进行扛压。比如下面图中示例的一种最简化版本的组网架构：\n\n开发人员对缓存进行了整改，将本地缓存改为了Redis集中式缓存。这样一来：\n\n缓存不一致问题解决：解决了各个节点间数据不一致的问题。\n\n单机内存容量限制解决：使用了Redis这种分布式的集中式缓存，扩大了内存缓存的容量范围，可以顺便将很多业务层面的数据全部加载到Redis中分片进行缓存，性能也相比而言得到了提升。\n\n\n似乎使用集中式缓存已经是分布式系统中的最优解了，但是现实情况真的就这么简单么？也不尽然！\n多级缓存的珠联璧合在尝到了集中式缓存的甜头之后，暖心的程序员们想到要彻底为数据库减压，将所有业务中需要频繁使用的数据全部同步存储到Redis中，然后业务使用的时候直接从Redis中获取相关数据，大大地减少了数据库的请求频次。但是改完上线之后，发现有些处理流程中并没有太大的性能提升。缘何如此？只因为对集中式缓存的过分滥用！分析发现这些流程的处理需要涉及大量的交互与数据整合逻辑，一个流程需要访问近乎30次Redis！虽然Redis的单次请求处理性能极高，甚至可以达到微秒级别的响应速度，但是每个流程里面几十次的网络IO交互，导致频繁的IO请求，以及线程的阻塞与唤醒切换交替，使得系统在线程上下文切换层面浪费巨大。\n那么，要想破局，最常规的手段便是尝试降低对集中式缓存（如Redis）的请求数量，降低网络IO交互次数。而如何来降低呢？ —— 又回到了本地缓存！集中式缓存并非是分布式系统中提升性能的银弹，但我们可以将本地缓存与集中式缓存结合起来使用，取长补短，实现效果最大化。如图所示：\n\n上图演示的也即多级缓存的策略。具体而言：\n\n对于一些变更频率比较高的数据，采用集中式缓存，这样可以确保数据变更之后所有节点都可以实时感知到，确保数据一致；\n\n对于一些极少变更的数据（比如一些系统配置项）或者是一些对短期一致性要求不高的数据（比如用户昵称、签名等）则采用本地缓存，大大减少对远端集中式缓存的网络IO次数。\n\n\n这样一来，系统的响应性能又得到了进一步的提升。\n通过对缓存使用策略的一步步演进，我们可以感受到缓存的恰当使用对系统性能的帮助作用。\n\n无处不在的缓存缓存存在的初衷，就是为了兼容两个处理速度不一致的场景对接适配的。在我们的日常生活中，也常常可以看到“缓存”的影子。比如对于几年前比较盛行的那种带桶的净水器（见下图），由于净水的功率比较小，导致实时过滤得到纯净水的水流特别的缓慢，用户倒一杯水要等2分钟，体验太差，所以配了个蓄水桶，净水机先慢慢的将净化后的水存储到桶中，然后用户倒水的时候可以从桶里快速的倒出，无需焦急等待 —— 这个蓄水桶，便是一个缓存器。\n\n编码源于生活，CPU的高速缓存设计就是这一生活实践在计算机领域的原样复制。缓存可以说在软件世界里无处不在，除了我们自己的业务系统外，在网络传输、操作系统、中间件、基础框架中都可以看到缓存的影子。如：\n\n网络传输场景。\n\n比如ARP协议，基于ARP缓存表进行IP与终端硬件MAC地址之间的缓存映射。这样与对端主机之间有通信需求的时候，就可以在ARP缓存中查找到IP对应的对端设备MAC地址，避免每次请求都需要去发送ARP请求查询MAC地址。\n\nMyBatis的多级缓存。\n\nMyBatis作为JAVA体系中被广泛使用的数据库操作框架，其内部为了提升处理效率，构建了一级缓存与二级缓存，大大减少了对SQL的重复执行次数。\n\nCPU中的缓存。\n\nCPU与内存之间有个临时存储器（高速缓存），容量虽比内存小，但是处理速度却远快于普通内存。高速缓存的机制，有效地解决了CPU运算速度与内存读写速度不匹配的问题。\n\n缓存的使用场景缓存作为互联网类软件系统架构与实现中的基石般的存在，不仅仅是在系统扛压或者接口处理速度提升等性能优化方案，在其他多个方面都可以发挥其独一无二的关键价值。下面就让我们一起来看看缓存都可以用在哪些场景上，可以解决我们哪方面的痛点。\n降低自身CPU消耗如前面章节中提到的项目实例，缓存最典型的使用场景就是用在系统的性能优化上。而在性能优化层面，一个经典的策略就是“空间换时间”。比如：\n\n在数据库表中做一些字段冗备。\n\n比如用户表T_User和部门表T_Department，在T_User表中除了有个Department_Id字段与T_Department表进行关联之外，还额外在T_User表中存储Department_Name值。这样在很多需要展示用户所属部门信息的时候就省去了多表关联查询的操作。\n\n\n对一些中间处理结果进行存储。\n\n比如系统中的数据报表模块，需要对整个系统内所有的关联业务数据进行计算统计，且需要多张表多来源数据之间的综合汇总之后才能得到最终的结果，整个过程的计算非常的耗时。如果借助缓存，则可以将一些中间计算结果进行暂存，然后报表请求中基于中间结果进行二次简单处理即可。这样可以大大降低基于请求触发的实时计算量。\n在“空间换时间”实施策略中，缓存是该策略的核心、也是被使用的最为广泛的一种方案。借助缓存，可以将一些CPU耗时计算的处理结果进行缓存复用，以降低重复计算工作量，达到降低CPU占用的效果。\n\n减少对外IO交互上面介绍的使用缓存是为了不断降低请求处理时对自身CPU占用，进而提升服务的处理性能。这里我们介绍缓存的另一典型使用场景，就是减少系统对外依赖的请求频次。即通过将一些从远端请求回来的响应结果进行缓存，后面直接使用此缓存结果而无需再次发起网络IO请求交互。\n对于服务端而言，通过构建缓存的方式来减少自身对外的IO请求，主要有几个考量出发点：\n\n从自身性能层面考虑，减少对外IO操作，降低了对外接口的响应时延，也对服务端自身处理性能有一定提升。\n\n从对端服务稳定性层面考虑，避免对端服务负载过大。很多时候调用方和被调用方系统的承压能力是不匹配的，甚至有些被调用方系统可能是不承压的。为了避免将对端服务压垮，需要调用方缓存请求结果，降低IO请求。\n\n从自身可靠性层面而言，将一些远端服务请求到的结果缓存起来，即使远端服务出现故障，自身业务依旧可以基于缓存数据进行正常业务处理，起到一个兜底作用，提升自身的抗风险能力。\n\n\n在分布式系统服务治理范畴内，服务注册管理服务是必不可少的，比如SpringCloud家族的Eureka，或者是Alibaba开源的Nacos。它们对于缓存的利用，可以说是对上面所提几点的完美阐述。\n以Nacos为例：\n\n除了上述的因素之外，对一些移动端APP或者H5界面而言，缓存的使用还有一个层面的考虑，即降低用户的流量消耗，通过将一些资源类数据缓存到本地，避免反复去下载，给用户省点流量，也可以提升用户的使用体验（界面渲染速度快，减少出现白屏等待的情况）。\n\n提升用户个性化体验缓存除了在系统性能提升或系统可靠性兜底等场景发挥价值外，在APP或者web类用户侧产品中，还经常被用于存储一些临时非永久的个性化使用习惯配置或者身份数据，以提升用户的个性化使用体验。\n\n缓存cookie、session等身份鉴权信息，这样就可以避免用户每次访问都需要进行身份验证。\n\n\n\n记住一些用户上次操作习惯，比如用户在一个页面上将列表分页查询设置为100条&#x2F;页，则后续在系统内访问其它列表页面时，都沿用这一设置。\n\n缓存用户的一些本地设置，这个主要是APP端常用的功能，可以在缓存中保存些与当前设备绑定的设置信息，仅对当前设备有效。比如同一个账号登录某个APP，用户希望在手机端可以显示深色主题，而PAD端则显示浅色主体，这种基于设备的个性化设置，可以缓存到设备本身即可。\n\n\n\n业务与缓存的集成模式如前所述，我们可以在不同的方面使用缓存来辅助达成项目在某些方面的诉求。而根据使用场景的不同，在结合缓存进行业务逻辑实现的时候，也会存在不同的架构模式，典型的会有旁路型缓存、穿透型缓存与异步型缓存三种。\n旁路型缓存在旁路型缓存模式中，业务自行负责与缓存以及数据库之间的交互，可以自由决定缓存未命中场景的处理策略，更加契合大部分业务场景的定制化诉求。\n\n由于业务模块自行实现缓存与数据库之间的数据写入与更新的逻辑，实际实现的时候需要注意下在高并发场景的数据一致性问题，以及可能会出现的缓存击穿、缓存穿透、缓存雪崩等问题的防护。\n旁路型缓存是实际业务中最常使用的一种架构模式，在后面的内容中，我们还会不断的涉及到旁路缓存中相关的内容。\n\n穿透型缓存穿透型缓存在实际业务中使用的较少，主要是应用在一些缓存类的中间件中，或者在一些大型系统中专门的数据管理模块中使用。\n一般情况下，业务使用缓存的时候，会是先尝试读取缓存，在尝试读取DB，而使用穿透型缓存架构时，会有专门模块将这些动作封装成黑盒的，业务模块不会与数据库进行直接交互。如下图所示：\n\n这种模式对业务而言是比较友好的，业务只需调用缓存接口即可，无需自行实现缓存与DB之间的交互策略。\n异步型缓存还有一种缓存的使用模式，可以看作是穿透型缓存的演进异化版本，其使用场景也相对较少，即异步型缓存。其主要策略就是业务侧请求的实时读写交互都是基于缓存进行，任何数据的读写也完全基于缓存进行操作。此外，单独实现一个数据持久化操作(独立线程或者进程中执行)，用于将缓存中变更的数据写入到数据库中。\n\n这种情况，实时业务读写请求完全基于缓存进行，而将数据库仅仅作为一个数据持久化存储的备份盘。由于实时业务请求仅与缓存进行交互，所以在性能上可以得到更好的表现。但是这种模式也存在一个致命的问题：数据可靠性！因为是异步操作，所以在下一次数据写入DB前，会有一段时间数据仅存在于缓存中，一旦缓存服务宕机，这部分数据将会丢失。所以这种模式仅适用于对数据一致性要求不是特别高的场景。\n缓存的优秀实践缓存与持久化存储的一个很大的不同点就是缓存的定位应该是一种辅助角色，是一种锦上添花般的存在。\n缓存也是一把双刃剑，基于缓存可以大幅提升我们的系统并发与承压能力，但稍不留神也可能会让我们的系统陷入灭顶之灾。所以我们在决定使用缓存的时候，需要知晓缓存设计与使用的一些关键要点，才可以让我们在使用的时候更加游刃有余。\n可删除重建可删除重建，这是缓存与持久化存储最大的一个差别。缓存的定位一定是为了辅助业务处理而生的，也就是说缓存有则使用，没有也不会影响到我们具体的业务运转。此外，即使我们的缓存数据除了问题，我们也可以将其删除重建。\n这一点在APP类的产品中体现的会比较明显。比如对于微信APP的缓存，就有明确的提示说缓存可以删除而不会影响其功能使用：\n\n同样地，我们也可以去放心的清理浏览器的缓存，而不用担心清理之后我们浏览器或者网页的功能会出现异常（最多就是需要重新下载或者重建缓存数据，速度会有一些慢）。\n\n相同的逻辑，在服务端构建的一些缓存，也应该具备此特性。比如基于内存的缓存，当业务进程重启后，应该有途径可以将缓存重建出来（比如从MySQL中加载数据然后构建缓存，或者是缓存从0开始基于请求触发而构建）。\n\n有兜底屏障缓存作为高并发类系统中的核心组件，负责抗住大部分的并发请求，一旦缓存组件出问题，往往对整个系统会造成毁灭性的打击。所以我们的缓存在实现的时候必须要有充足且完备的兜底与自恢复机制。需要做到以下几点：\n\n关注下缓存数据量超出承受范围的处理策略，比如定好数据的淘汰机制。\n\n避免缓存集中失效，比如批量加载数据到缓存的时候随机打散过期时间，避免同一时间大批量缓存失效引发缓存雪崩问题。\n\n有效地冷数据预热加载机制，以及热点数据防过期机制，避免出现大量对冷数据的请求无法命中缓存或者热点数据突然失效，导致缓存击穿问题。\n\n合理的防身自保手段，比如采用布隆过滤器机制，避免被恶意请求攻陷，导致缓存穿透类的问题。\n\n\n缓存的可靠性与兜底策略设计，是一个宏大且宽泛的命题，在本系列专栏后续的文章中，我们会逐个深入的探讨。\n\n关注缓存的一致性保证在高并发类的系统中进行数据更新的时候，缓存与数据库的数据一致性问题，是一个永远无法绕过的话题。对于基于旁路型缓存的大部分业务而言，数据更新操作，一般可以组合出几种不同的处理策略：\n\n先更新缓存，再更新数据库\n\n先更新数据库， 再更新缓存\n\n先删除缓存，再更新数据库\n\n先更新数据库，再删除缓存\n\n\n由于大部分数据库都支持事务，而几乎所有的缓存操作都不具有事务性。所以在一些写操作并发不是特别高且一致性要求不是特别强烈的情况下，可以简单的借助数据库的事务进行控制。比如先更新数据库再更新缓存，如果缓存更新失败则回滚数据库事务。\n然而在一些并发请求特别高的时候，基于事务控制来保证数据一致性往往会对性能造成影响，且事务隔离级别设置的越高影响越大，所以也可以采用一些其它辅助策略，来替代事务的控制，如重试机制、或异步补偿机制、或多者结合方式等。\n比如下图所示的这种策略：\n\n上图的数据更新处理策略，可以有效地保证数据的最终一致性，降低极端情况可能出现数据不一致的概率，并兜底增加了数据不一致时的自恢复能力。\n数据一致性保证作为缓存的另一个重要命题，我们会在本系列专栏后续的文章中专门进行深入的剖析。\n\n总结回顾本篇文章的内容中，我们对缓存的各个方面进行了一个简单的阐述与了解，也可以看出缓存对于一个软件系统的重要价值。通过对缓存的合理、充分利用，可以大大的增强我们的系统承压性能、提升产品的用户体验。\n缓存作为高并发系统中的神兵利器被广泛使用，堪称高并发系统的基石之一。而缓存的内容还远远不止我们本篇文档中所介绍的这些、它是一个非常宏大的命题。\n\n为了能够将缓存的方方面面彻底的讲透、讲全，在接下来的一段时间里，我会以系列专栏的形式，从不同的角度对缓存的方方面面进行探讨。不仅仅着眼于如何去使用缓存、也一起聊聊缓存设计中的一些哲学理念 —— 这一点是我觉得更有价值的一点，因为这些理念对提升我们的软件架构认知、完善我们的软件设计思维有很大的指导与借鉴意义。\n所以，如果你有兴趣，欢迎关注本系列专栏（深入理解缓存原理与实战设计），我会以我一贯的行文风格，用最简单的语言讲透复杂的逻辑，期待一起切磋、共同成长。\n\n我是悟道，聊技术、又不仅仅聊技术~\n如果觉得有用，请点赞 + 关注让我感受到您的支持。也可以关注下我的公众号【架构悟道】，获取更及时的更新。\n期待与你一起探讨，一起成长为更好的自己。\n\n\n","slug":"2023/hello-world3","date":"2022-11-16T16:00:00.000Z","categories_index":"缓存实践专栏1","tags_index":"缓存,JAVA,高并发,高负载","author_index":"Veezean"},{"id":"3ff8444651675b17f1511535bbc99f8d","title":"Hello World2","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n","slug":"2022/hello-world2","date":"2022-11-14T16:00:00.000Z","categories_index":"缓存实践专栏2","tags_index":"缓存,JAVA,高并发,高负载","author_index":"Veezean"}]